#!/usr/bin/python3

import argparse
import getpass
import os
import sys
from multiprocessing import Pool
import subprocess
import boto3
from boto3.exceptions import S3UploadFailedError

do_upload = '--no-upload' not in sys.argv

parser = argparse.ArgumentParser()
parser.add_argument('--path', type=str,
                    help='Path to root folder containing subfolders to be archived. Defaults to CWD.', required=False)
parser.add_argument('--exclude', type=str, nargs='+',
                    help='List of subfolders to exclude', required=False)
if do_upload:
    parser.add_argument('--bucket', type=str,
                    help='S3 bucket name to upload to', required=True)
    parser.add_argument('--region', type=str,
                        help='S3 region that the bucket is located in', required=True)
    parser.add_argument('--folder', type=str,
                        help='Remote folder path', required=True)
args = parser.parse_args()

if not args.path:
    folders = os.listdir()
else:
    folders = os.listdir(args.path)

if args.exclude:
    folders = [f for f in folders if f not in args.exclude]

encryption_key = getpass.getpass()

if do_upload:
    s3_key_file = f'{os.getenv("HOME")}/.s3.key'
    with open(s3_key_file, 'r') as file:
        data = file.read().rstrip('\n')
    access_key_id, secret_access_key = data.split(':')
    s3 = boto3.resource('s3', region_name=args.region,
                        aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key)


def archive_upload(folder):
    print(f'Compressing {folder}...')
    filename = f'{folder.replace(" ", "_")}.tar.gz.asc'
    if args.path:
        os.chdir(args.path)
    tar = subprocess.Popen(
        ('tar', '-cvzf', '-', folder), stdout=subprocess.PIPE)
    gpg = subprocess.Popen(
        ('gpg', '--symmetric', '--output', f'/tmp/{filename}', '--batch', '--passphrase', encryption_key), stdin=tar.stdout)
    if tar.wait() or gpg.wait():
        print(f'Error compressing "{folder}"')
        os.remove(f'/tmp/{filename}')
        return

    if do_upload:
        print(f'Uploading {filename}...')
        try:
            s3.Bucket(args.bucket).upload_file(
                f'/tmp/{filename}', f'{args.folder}/{filename}', ExtraArgs={'StorageClass': 'DEEP_ARCHIVE'})
            print(f'Successfully uploaded {filename}')
        except S3UploadFailedError as err:
            print(str(err))
        finally:
            os.remove(f'/tmp/{filename}')

if __name__ == '__main__':
    with Pool(len(folders)) as p:
        p.map(archive_upload, folders)
