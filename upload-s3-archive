#!/usr/bin/python3

import argparse
import getpass
import os
import sys
from multiprocessing import Pool
import subprocess
import boto3
from boto3.exceptions import S3UploadFailedError

parser = argparse.ArgumentParser()
parser.add_argument('--path', type=str,
                    help='Path to root folder containing subfolders to be archived. Defaults to CWD.', required=False)
parser.add_argument('--exclude', type=str, nargs='+',
                    help='List of subfolders to exclude', required=False)
parser.add_argument('--bucket', type=str,
                    help='S3 bucket name to upload to', required=True)
parser.add_argument('--region', type=str,
                    help='S3 region that the bucket is located in', required=True)
parser.add_argument('--folder', type=str,
                    help='Remote folder path', required=True)
args = parser.parse_args()

if not args.path:
    folders = os.listdir()
else:
    folders = os.listdir(args.path)

if args.exclude:
    folders = [f for f in folders if f not in args.exclude]

encryption_key = getpass.getpass()

s3_key_file = f'{os.getenv("HOME")}/.s3.key'
with open(s3_key_file, 'r') as file:
    data = file.read().rstrip('\n')
access_key_id, secret_access_key = data.split(':')
s3 = boto3.resource('s3', region_name=args.region,
                    aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key)

def archive_upload(folder):
    print(f'Compressing {folder}...')
    filename = f'{folder.replace(" ", "_")}.tar.gz.asc'
    if args.path:
        os.chdir(args.path)
    tar = subprocess.Popen(
        ('tar', '-cvzf', '-', folder), stdout=subprocess.PIPE)
    gpg = subprocess.Popen(
        ('gpg', '--symmetric', '--output', f'/tmp/{filename}', '--batch', '--passphrase', encryption_key), stdin=tar.stdout)
    if tar.wait() or gpg.wait():
        print(f'Error compressing "{folder}"')
        os.remove(f'/tmp/{filename}')
        return

    print(f'Uploading {filename}...')
    try:
        s3.Bucket(args.bucket).upload_file(
            f'/tmp/{filename}', f'{args.folder}/{filename}', ExtraArgs={'StorageClass': 'DEEP_ARCHIVE'})
        print(f'Successfully uploaded {filename}')
    except S3UploadFailedError as err:
        print(str(err))
    finally:
        os.remove(f'/tmp/{filename}')


if __name__ == '__main__':
    with Pool(len(folders)) as p:
        p.map(archive_upload, folders)
